# Shell脚本日志分析

## 概述
日志分析是系统管理中的重要技能，Shell脚本提供了强大的文本处理能力来分析各种日志文件。

## 常用日志分析命令

### 1. 基本日志查看
```bash
# 查看最新日志
tail -f /var/log/syslog

# 查看指定行数
tail -n 100 /var/log/apache2/access.log

# 查看日志头部
head -n 50 /var/log/messages

# 实时监控多个日志文件
tail -f /var/log/syslog /var/log/auth.log
```

### 2. 日志过滤和搜索
```bash
# 搜索错误信息
grep -i "error" /var/log/syslog

# 搜索特定时间段
grep "2024-01-15" /var/log/apache2/access.log

# 使用正则表达式
grep -E "404|500|502" /var/log/nginx/access.log

# 反向搜索（排除）
grep -v "INFO" /var/log/application.log
```

## 实用日志分析脚本

### 1. Apache访问日志分析器
```bash
#!/bin/bash

# Apache访问日志分析脚本
LOG_FILE="/var/log/apache2/access.log"

echo "=== Apache访问日志分析报告 ==="
echo "分析文件: $LOG_FILE"
echo "生成时间: $(date)"
echo

# 总访问量
echo "1. 总访问量:"
wc -l < "$LOG_FILE"
echo

# 独立IP统计
echo "2. 独立IP访问量:"
awk '{print $1}' "$LOG_FILE" | sort | uniq | wc -l
echo

# 访问量最多的IP（前10）
echo "3. 访问量最多的IP（前10）:"
awk '{print $1}' "$LOG_FILE" | sort | uniq -c | sort -nr | head -10
echo

# 最受欢迎的页面（前10）
echo "4. 最受欢迎的页面（前10）:"
awk '{print $7}' "$LOG_FILE" | sort | uniq -c | sort -nr | head -10
echo

# HTTP状态码统计
echo "5. HTTP状态码统计:"
awk '{print $9}' "$LOG_FILE" | sort | uniq -c | sort -nr
echo

# 每小时访问量统计
echo "6. 每小时访问量统计:"
awk '{print $4}' "$LOG_FILE" | cut -c 14-15 | sort | uniq -c
echo

# 用户代理统计（前5）
echo "7. 用户代理统计（前5）:"
awk -F'"' '{print $6}' "$LOG_FILE" | sort | uniq -c | sort -nr | head -5
```

### 2. 系统日志错误监控
```bash
#!/bin/bash

# 系统日志错误监控脚本
SYSLOG="/var/log/syslog"
ERROR_LOG="/tmp/system_errors.log"
ALERT_EMAIL="admin@example.com"

# 检查错误关键词
ERROR_KEYWORDS=("error" "failed" "critical" "panic" "segfault")

echo "=== 系统错误监控报告 ===" > "$ERROR_LOG"
echo "检查时间: $(date)" >> "$ERROR_LOG"
echo >> "$ERROR_LOG"

# 检查最近1小时的错误
SINCE_TIME=$(date -d '1 hour ago' '+%b %d %H:%M')

for keyword in "${ERROR_KEYWORDS[@]}"; do
    echo "检查关键词: $keyword" >> "$ERROR_LOG"
    grep -i "$keyword" "$SYSLOG" | grep "$SINCE_TIME" >> "$ERROR_LOG"
    echo >> "$ERROR_LOG"
done

# 如果发现错误，发送邮件警报
if [ -s "$ERROR_LOG" ]; then
    echo "发现系统错误，发送警报邮件..."
    mail -s "系统错误警报 - $(hostname)" "$ALERT_EMAIL" < "$ERROR_LOG"
fi

# 显示结果
cat "$ERROR_LOG"
```

### 3. 日志轮转检查脚本
```bash
#!/bin/bash

# 日志轮转检查脚本
LOG_DIRS=("/var/log" "/var/log/apache2" "/var/log/nginx")
MAX_SIZE_MB=100
REPORT_FILE="/tmp/log_rotation_report.txt"

echo "=== 日志轮转检查报告 ===" > "$REPORT_FILE"
echo "检查时间: $(date)" >> "$REPORT_FILE"
echo "最大允许大小: ${MAX_SIZE_MB}MB" >> "$REPORT_FILE"
echo >> "$REPORT_FILE"

check_log_size() {
    local log_file="$1"
    local size_mb=$(du -m "$log_file" 2>/dev/null | cut -f1)
    
    if [ "$size_mb" -gt "$MAX_SIZE_MB" ]; then
        echo "警告: $log_file 大小为 ${size_mb}MB，超过限制" >> "$REPORT_FILE"
        return 1
    else
        echo "正常: $log_file 大小为 ${size_mb}MB" >> "$REPORT_FILE"
        return 0
    fi
}

# 检查各个日志目录
for dir in "${LOG_DIRS[@]}"; do
    if [ -d "$dir" ]; then
        echo "检查目录: $dir" >> "$REPORT_FILE"
        find "$dir" -name "*.log" -type f | while read -r logfile; do
            check_log_size "$logfile"
        done
        echo >> "$REPORT_FILE"
    fi
done

# 显示报告
cat "$REPORT_FILE"
```

## 高级日志分析技巧

### 1. 使用awk进行复杂分析
```bash
# 分析nginx访问日志，统计每个IP的带宽使用
awk '{
    ip = $1
    bytes = $10
    if (bytes ~ /^[0-9]+$/) {
        total[ip] += bytes
        count[ip]++
    }
} 
END {
    for (ip in total) {
        printf "%s: %d bytes (%d requests)\n", ip, total[ip], count[ip]
    }
}' /var/log/nginx/access.log | sort -k2 -nr
```

### 2. 日志实时分析
```bash
#!/bin/bash

# 实时日志分析脚本
LOG_FILE="/var/log/apache2/access.log"

echo "开始实时监控 $LOG_FILE"
echo "按 Ctrl+C 停止监控"

tail -f "$LOG_FILE" | while read line; do
    # 提取IP地址
    ip=$(echo "$line" | awk '{print $1}')
    
    # 提取状态码
    status=$(echo "$line" | awk '{print $9}')
    
    # 提取请求时间
    timestamp=$(echo "$line" | awk '{print $4}' | tr -d '[')
    
    # 检查异常状态码
    case "$status" in
        404)
            echo "[$(date)] 404错误 - IP: $ip"
            ;;
        500|502|503)
            echo "[$(date)] 服务器错误 $status - IP: $ip"
            ;;
        200)
            # 正常请求，可以选择不显示
            ;;
        *)
            echo "[$(date)] 状态码 $status - IP: $ip"
            ;;
    esac
done
```

### 3. 日志统计报告生成器
```bash
#!/bin/bash

# 综合日志统计报告生成器
generate_report() {
    local log_file="$1"
    local report_file="$2"
    
    echo "=== 日志分析报告 ===" > "$report_file"
    echo "日志文件: $log_file" >> "$report_file"
    echo "生成时间: $(date)" >> "$report_file"
    echo "分析期间: $(head -1 "$log_file" | awk '{print $4}') 到 $(tail -1 "$log_file" | awk '{print $4}')" >> "$report_file"
    echo >> "$report_file"
    
    # 基本统计
    echo "=== 基本统计 ===" >> "$report_file"
    echo "总请求数: $(wc -l < "$log_file")" >> "$report_file"
    echo "独立IP数: $(awk '{print $1}' "$log_file" | sort | uniq | wc -l)" >> "$report_file"
    echo >> "$report_file"
    
    # 状态码分布
    echo "=== HTTP状态码分布 ===" >> "$report_file"
    awk '{print $9}' "$log_file" | sort | uniq -c | sort -nr >> "$report_file"
    echo >> "$report_file"
    
    # 热门页面
    echo "=== 热门页面 (前10) ===" >> "$report_file"
    awk '{print $7}' "$log_file" | sort | uniq -c | sort -nr | head -10 >> "$report_file"
    echo >> "$report_file"
    
    # 访问量最大的IP
    echo "=== 访问量最大的IP (前10) ===" >> "$report_file"
    awk '{print $1}' "$log_file" | sort | uniq -c | sort -nr | head -10 >> "$report_file"
    echo >> "$report_file"
    
    echo "报告已生成: $report_file"
}

# 使用示例
if [ $# -eq 0 ]; then
    echo "用法: $0 <日志文件> [报告文件]"
    exit 1
fi

LOG_FILE="$1"
REPORT_FILE="${2:-/tmp/log_report_$(date +%Y%m%d_%H%M%S).txt}"

if [ ! -f "$LOG_FILE" ]; then
    echo "错误: 日志文件 $LOG_FILE 不存在"
    exit 1
fi

generate_report "$LOG_FILE" "$REPORT_FILE"
```

## 性能优化技巧

### 1. 大文件处理
```bash
# 对于大型日志文件，使用更高效的方法
# 使用sed而不是grep处理大文件
sed -n '/ERROR/p' large_log.log

# 使用split分割大文件
split -l 10000 large_log.log chunk_

# 并行处理
find . -name "chunk_*" | xargs -P 4 -I {} grep "ERROR" {}
```

### 2. 内存优化
```bash
# 避免将整个文件加载到内存
# 使用管道和流处理
cat large_log.log | grep "ERROR" | head -100

# 使用while循环逐行处理
while IFS= read -r line; do
    # 处理每一行
    echo "$line" | grep "ERROR"
done < large_log.log
```

## 实践练习

1. 编写脚本分析Web服务器访问日志，找出访问量异常的IP地址
2. 创建系统日志监控脚本，当出现特定错误时发送邮件通知
3. 实现日志文件自动压缩和清理功能
4. 编写脚本统计应用程序的错误率和响应时间

## 总结

日志分析是系统运维的核心技能，通过Shell脚本可以：
- 自动化日志监控和分析
- 生成详细的统计报告
- 实时检测异常情况
- 优化系统性能

掌握这些技巧将大大提高运维效率和系统可靠性。